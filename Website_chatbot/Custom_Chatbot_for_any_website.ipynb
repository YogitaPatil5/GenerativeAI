{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhAxudvcJmL1"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#Install All Required Packages\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zR5VkRe-8WAU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.90.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\yogit\\appdata\\roaming\\python\\python310\\site-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\yogit\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\yogit\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Requirement already satisfied: tiktoken in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n",
            "Requirement already satisfied: transformers in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yogit\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\yogit\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: langchain-community in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.16)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.1.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (3.12.13)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.16 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.32 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (0.3.66)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (0.3.45)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (1.24.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (2.10.0)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (2.32.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain<0.4.0,>=0.3.16->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\yogit\\appdata\\roaming\\python\\python310\\site-packages (from langchain-core<0.4.0,>=0.3.32->langchain-community) (4.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: anyio in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.16->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (0.33.0)\n",
            "Requirement already satisfied: Pillow in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\yogit\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\yogit\\appdata\\roaming\\python\\python310\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.21.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tokenizers) (0.33.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.5.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yogit\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.14.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\yogit\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.6.15)\n",
            "Requirement already satisfied: faiss-cpu in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from faiss-cpu) (1.24.4)\n",
            "Requirement already satisfied: packaging in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: numpy==1.24.4 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.24.4)\n",
            "Requirement already satisfied: nltk==3.9.1 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: click in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk==3.9.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk==3.9.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk==3.9.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\yogit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk==3.9.1) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\yogit\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk==3.9.1) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "# !pip -q install langchain langchain-community\n",
        "# !pip -q install pypdf\n",
        "# !pip -q install sentence_transformers\n",
        "# !pip install openai\n",
        "# !pip install tiktoken\n",
        "# !pip install transformers\n",
        "# !pip install langchain-community sentence-transformers\n",
        "# !pip install tokenizers\n",
        "# !pip install faiss-cpu\n",
        "# !pip -q install unstructured\n",
        "# !pip install numpy==1.24.4\n",
        "# !pip install nltk==3.9.1\n",
        "# !pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57qKZ0-IKYf9"
      },
      "source": [
        "#Import all the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9TiWno_RKLCX"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "#import torch\n",
        "import textwrap\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxR6theDMIk6",
        "outputId": "ed9d7643-dd03-41f3-f9de-e1ba1cf9887b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\yogit\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Error loading average_perceptron_tagger: Package\n",
            "[nltk_data]     'average_perceptron_tagger' not found in index\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('average_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "MjRLKTbdMwT1"
      },
      "outputs": [],
      "source": [
        "# # from google.colab import userdata\n",
        "# # #os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# #os.environ['OPENAI_API_KEY'] =\" \"\n",
        "\n",
        "# # Set OpenAI API Key securely\n",
        "# if 'OPENAI_API_KEY' not in os.environ:\n",
        "#     os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API Key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEgl1yWXQSBd"
      },
      "source": [
        "#Pass the URL and extract the data from these URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DVyVn3INPkXP"
      },
      "outputs": [],
      "source": [
        "URLs = [\n",
        "    'https://docs.aws.amazon.com/',\n",
        "    'https://stability.ai/news/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models',\n",
        "    'https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html',\n",
        "    'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb',\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_pc8P_0JSDNo"
      },
      "outputs": [],
      "source": [
        "loaders = UnstructuredURLLoader(urls=URLs)\n",
        "data= loaders.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAf_W7ypSOWm",
        "outputId": "4f1dd6dd-4bb3-4fc1-fb96-1c2c93d1a320"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://docs.aws.amazon.com/'}, page_content=''),\n",
              " Document(metadata={'source': 'https://stability.ai/news/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models'}, page_content='Stability AI Launches the First of its Stable LM Suite of Language Models\\n\\nProduct\\n\\nApr 19\\n\\nWritten By Joshua Lopez\\n\\nToday, Stability AI released a new open source language model, Stable LM. The Alpha version of the model is available in 3 billion and 7 billion parameters, with 15 billion to 65 billion parameter models to follow. Developers can freely inspect, use, and adapt our Stable LM base models for commercial or research purposes, subject to the terms of the CC BY-SA-4.0 license.\\n\\nIn 2022, Stability AI drove the public release of Stable Diffusion, a revolutionary image model representing a transparent, open, and scalable alternative to proprietary AI. With the launch of the Stable LM suite of models, Stability AI is continuing to make foundational AI technology accessible to all. Our Stable LM models can generate text and code and will power various downstream applications. They demonstrate how small and efficient models can deliver high performance with appropriate training.\\n\\nThe release of Stable LM builds on our experience in open sourcing earlier language models with EleutherAI, a nonprofit research hub. These language models include GPT-J, GPT-NeoX, and the Pythia suite, trained on The Pile open source dataset. Many recent open source language models continue to build on these efforts, including Cerebras-GPT and Dolly-2.\\n\\nStable LM is trained on a new experimental dataset built on The Pile, but three times larger with 1.5 trillion content tokens. We will release details on the dataset in due course. The richness of this dataset gives Stable LM surprisingly high performance in conversational and coding tasks despite its small size of 3 to 7 billion parameters (by comparison, GPT-3 has 175 billion parameters).\\n\\nWe are also releasing a set of research models that are fine-tuned instructions. Initially, these fine-tuned models will use a combination of five recent open source datasets for conversational agents: Alpaca, GPT4All, Dolly, ShareGPT, and HH. These fine-tuned models are intended for research use only and are released under a noncommercial CC BY-NC-SA 4.0 license, in line with Stanford’s Alpaca license.\\n\\nCheck out some examples below, produced by our 7 billion parameters fine-tuned model:\\n\\nView fullsize\\n\\nexample 1.png\\n\\nView fullsize\\n\\nexample 2.png\\n\\nView fullsize\\n\\nexample 3.png\\n\\nLanguage models will form the backbone of our digital economy, and we want everyone to have a voice in their design. Models like Stable LM demonstrate our commitment to AI technology that is transparent, accessible, and supportive:\\n\\nTransparent. We open source our models to promote transparency and foster trust. Researchers can “look under the hood” to verify performance, work on interpretability techniques, identify potential risks, and help develop safeguards. Organizations across the public and private sectors can adapt (“fine-tune”) these open source models for their own applications without sharing their sensitive data or giving up control of their AI capabilities.\\n\\nAccessible. We design for the edge so everyday users can run our models on local devices. Using these models, developers can build independent applications compatible with widely available hardware instead of relying on proprietary services from one or two companies. In this way, the economic benefits of AI are shared by a broad community of users and developers. Open, fine-grained access to our models allows the broad research and academic community to develop interpretability and safety techniques beyond what is possible with closed models.\\n\\nSupportive. We build models to support our users, not replace them. We are focused on efficient, specialized, and practical AI performance – not a quest for god-like intelligence. We develop tools that help everyday people and everyday firms use AI to unlock creativity, boost their productivity, and open up new economic opportunities.\\n\\nThe models are now available in our GitHub repository. We will publish a full technical report in the near future and look forward to ongoing collaboration with developers and researchers as we roll out the Stable LM suite. In addition, we will be kicking off our crowd-sourced RLHF program and working with community efforts such as Open Assistant to create an open source dataset for AI assistants.\\n\\nWe will be releasing more models soon and are growing our team. If you are passionate about democratizing access to this technology and experienced in LLMs, please apply here!\\n\\n\\n\\nJoshua Lopez\\n\\nPrevious\\n\\nPrevious\\n\\nStability AI releases its Image Upscaling API\\n\\nNext\\n\\nNext\\n\\nStability AI Partners with Iconic Artist Peter Gabriel to Launch Series of AI Animation Challenges titled #DiffuseTogether'),\n",
              " Document(metadata={'source': 'https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html'}, page_content=''),\n",
              " Document(metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}, page_content='Sitemap\\n\\nOpen in app\\n\\nSign up\\n\\nSign in\\n\\nWrite\\n\\nSign up\\n\\nSign in\\n\\nGoPenAI\\n\\nFollow publication\\n\\nGoPenAI\\n\\nWhere the ChatGPT community comes together to share insights and stories.\\n\\nFollow publication\\n\\nPaper Review\\n\\nPaper Review: Llama 2: Open Foundation and Fine-Tuned Chat Models\\n\\nLlama 2: one of the best open source models\\n\\nAndrew Lukyanenko\\n\\nAndrew Lukyanenko\\n\\n15 min read\\n\\nJul 20, 2023\\n\\n--\\n\\nProject link\\n\\nModel link\\n\\nPaper link\\n\\nThe authors of the work present Llama 2, an assortment of pretrained and fine-tuned large language models (LLMs) with sizes varying from 7 billion to 70 billion parameters. The fine-tuned versions, named Llama 2-Chat, are specifically designed for dialogue applications. These models surpass the performance of existing open-source chat models on most benchmarks, and according to human evaluations for usefulness and safety, they could potentially replace closed-source models. The authors also detail their approach to fine-tuning and safety enhancements for Llama 2-Chat to support the community in further developing and responsibly handling LLMs.\\n\\nPretraining\\n\\nThe authors developed the Llama 2 model family starting from the pretraining methodology of Llama, which utilizes an optimized auto-regressive transformer. They implemented several modifications for improved performance, including enhanced data cleaning, updated data mixes, training on 40% more total tokens, and doubling the context length. They also incorporated grouped-query attention (GQA) to enhance the inference scalability for their larger models.\\n\\nPretraining Data\\n\\nThe authors utilized a novel mix of data from publicly accessible sources to train the Llama 2 models, excluding any data from Meta’s products or services. They made efforts to erase data from certain sites known for harboring large amounts of personal information about private individuals. They trained the models on 2 trillion tokens of data, believing this amount provided a beneficial performance-cost balance. They also up-sampled the most factual sources to boost knowledge and reduce instances of false information generation or “hallucinations”.\\n\\nLlama 2 Pretrained Model Evaluation\\n\\nLlama 2 models significantly outperform their Llama 1 counterparts:\\n\\nThe 70 billion-parameter Llama 2 model notably improves results on the MMLU and BBH benchmarks by roughly 5 and 8 points, respectively, when compared to the 65 billion-parameter Llama 1 model.\\n\\nLlama 2 models with 7 billion and 30 billion parameters outdo MPT models of similar size in all categories except code benchmarks.\\n\\nIn comparison with Falcon models, Llama 2’s 7 billion and 34 billion parameter models outperform the 7 billion and 40 billion parameter Falcon models in all benchmark categories.\\n\\nMoreover, the Llama 2 70B model surpasses all open-source models.\\n\\nComparatively, the Llama 2 70B model performs similarly to the closed-source GPT-3.5 (OpenAI, 2023) on the MMLU and GSM8K benchmarks but shows a significant deficit on coding benchmarks. It matches or exceeds the performance of PaLM (540 billion parameters) on nearly all benchmarks. However, there remains a substantial performance gap between the Llama 2 70B model and both GPT-4 and PaLM-2-L.\\n\\nFine-tuning\\n\\nSupervised Fine-Tuning (SFT)\\n\\nThe authors initiated the Supervised Fine-Tuning (SFT) phase using publicly available instruction tuning data like in Llama. However, they observed that many third-party SFT data sources lacked diversity and quality, particularly for aligning Large Language Models (LLMs) towards dialogue-style instructions. Therefore, they prioritized collecting several thousand high-quality SFT data examples, and found that using fewer but better-quality examples led to notable performance improvements.\\n\\nThe authors discovered that tens of thousands of SFT annotations were enough to achieve high-quality results, and ceased after collecting 27,540 annotations. They highlighted the significant impact of different annotation platforms and vendors on model performance, emphasizing the need for data checks even when sourcing annotations from vendors. A manual examination of a set of 180 examples showed that model outputs were often competitive with those handwritten by human annotators, suggesting the value in shifting more annotation efforts to preference-based annotation for Reinforcement Learning from Human Feedback (RLHF).\\n\\nIn fine-tuning, each sample consisted of a prompt and an answer, concatenated together with a special token used to separate the segments. The authors used an autoregressive objective and zeroed-out the loss on tokens from the user prompt, meaning they only backpropagated on answer tokens.\\n\\nHuman Preference Data Collection\\n\\nThe authors used a binary comparison protocol to collect human preference data for reward modeling in order to maximize the diversity of prompts. Annotators were tasked with writing a prompt and choosing between two model responses based on set criteria. These responses were sampled from two different model variants and varied by temperature hyperparameter. Annotators also had to rate their preference for the chosen response over the alternative (significantly better, better, slightly better, or negligibly better/ unsure.).\\n\\nThe focus of these preference annotations was on “helpfulness” and “safety”. The authors define “helpfulness” as how well Llama 2-Chat responses fulfil users’ requests, and “safety” as whether responses comply with their safety guidelines. Separate guidelines were provided for each focus area.\\n\\nDuring the safety stage, model responses were categorized into three groups: 1) the preferred response is safe and the other is not, 2) both responses are safe, and 3) both responses are unsafe. No examples where the chosen response was unsafe and the other safe were included.\\n\\nHuman annotations were collected in weekly batches. As more preference data was collected, their reward models improved, allowing the authors to train progressively better versions of Llama 2-Chat. As improvements shifted the model’s data distribution, the authors collected new preference data using the latest Llama 2-Chat iterations to keep the reward model up-to-date and accurate.\\n\\nThe authors collected over 1 million binary comparisons, referred to as Meta reward modeling data. Compared to existing open-source datasets, their preference data has more conversation turns and is, on average, longer.\\n\\nReward Modeling\\n\\nThe authors developed a reward model that inputs a model response and corresponding prompt and outputs a score indicating the quality (e.g., helpfulness, safety) of the generated response. These scores can then be used as rewards to optimize the Llama 2-Chat model for better alignment with human preferences.\\n\\nThey trained two separate reward models: one optimized for helpfulness (Helpfulness RM) and another for safety (Safety RM). These models were initialized from pre-trained chat model checkpoints, ensuring knowledge transfer and preventing discrepancies such as favoring hallucinations.\\n\\nTo train the reward model, pairwise human preference data was converted into a binary ranking label format. The model was trained to ensure that the chosen response scored higher than its counterpart. To leverage the four-point preference rating scale, they added a margin component to the loss to help the model assign more distinct scores to responses that have more differences.\\n\\nThe authors combined their new data with existing open-source preference datasets to form a larger training dataset.\\n\\nReward Model Results\\n\\nThe authors evaluated their reward models on a test set held out from each batch of human preference annotation data. They compared their models with publicly available alternatives, including SteamSHP-XL, the Open Assistant reward model, and GPT-4. Their models performed the best, particularly on the corresponding internal test sets.\\n\\nThe authors noted a tension between the goals of helpfulness and safety, and suggested this might be why their separate models performed best on their own domain. A single model that aims to perform well on both aspects would need to differentiate between better responses and distinguish safe prompts from adversarial ones.\\n\\nWhen scoring by preference rating, accuracy was superior for “significantly better” test sets, and it degraded as comparisons became more similar. The authors pointed out that accuracy on more distinct responses is key to improving Llama 2-Chat’s performance.\\n\\nIn terms of scaling trends, the authors found that larger models provided better performance for similar volumes of data, and performance had not plateaued with the current volume of annotated data. The authors concluded that improving the reward model’s accuracy could directly improve Llama 2-Chat’s performance, as the ranking task of the reward is unambiguous.\\n\\nIterative Fine-Tuning\\n\\nTwo main algorithms were used for RLHF fine-tuning: Proximal Policy Optimization (PPO), standard in RLHF literature, and Rejection Sampling fine-tuning, where they selected the best output from sampled model responses for a gradient update. The differences between the two algorithms lie in breadth (K samples for a given prompt in Rejection Sampling) and depth (in PPO the sample is a function of the updated model policy from the previous step, in RS all outputs are sampled. In iterative training the differences are less pronounced).\\n\\nRejection Sampling fine-tuning was performed only with the largest 70B Llama 2-Chat, with smaller models fine-tuned on the sampled data from the larger model. Over iterations, the authors adjusted their strategy to include top-performing samples from all prior iterations, leading to significant performance improvements.\\n\\nThe authors illustrate the benefit of Rejection Sampling in two ways. They show the delta between the maximum and median curves can be interpreted as the potential gain of fine-tuning on the best output. They also found that the optimal temperature for generating diverse samples isn’t constant during iterative model updates.\\n\\nAfter RLHF (V4), the authors sequentially combined Rejection Sampling and PPO fine-tuning. For PPO, they iteratively improved the policy by sampling prompts and generations from the policy and used the PPO algorithm to achieve the objective. They also added a penalty term for diverging from the original policy, as it’s helpful for training stability and to reduce reward hacking.\\n\\nSystem Message for Multi-Turn Consistency\\n\\nThe authors proposed Ghost Attention (GAtt), a technique designed to help AI remember initial instructions throughout a dialogue. This method, which builds on the concept of Context Distillation, introduces an instruction that needs to be followed throughout the conversation and is appended to all user messages in a synthetic dialogue dataset. During training, the instruction is only kept in the first turn and the loss is set to zero for all tokens from prior turns. This strategy was applied to a range of synthetic constraints including hobbies, language, and public figures. The implementation of GAtt helped maintain attention towards initial instructions over a larger part of the dialogue.\\n\\nGAtt managed to ensure consistency even over 20+ turns, until the maximum context length was reached. While this initial implementation has been beneficial, the authors believe there is potential for further enhancement and iteration on this technique.\\n\\nRLHF Results\\n\\nModel-Based Evaluation\\n\\nEvaluating large language models (LLMs) like Llama 2-Chat is a complex problem. While human evaluation is considered the gold standard, it is not always scalable and may present complications. As a solution, the authors first used reward models to measure improvement in iterations of their Reinforcement Learning from Human Feedback (RLHF) model versions, and later confirmed these findings with human evaluations.\\n\\nTo test the reward model’s reliability, the authors collected a test set of prompts and had them judged by human annotators. The results indicated that the reward models were generally well-aligned with human preferences, validating their use as a point-wise metric.\\n\\nHowever, to prevent possible divergence from human preferences, the authors also utilized a more general reward model trained on diverse open-source Reward Modeling datasets. They hypothesize that iterative model updates may help maintain alignment with human preferences.\\n\\nIn a final check to ensure no regression between new and old models, both models were used in the next annotation iteration for comparison.\\n\\nThe authors’ models were shown to outperform ChatGPT in both safety and helpfulness after RLHF-V3. For fair comparison, final results were also assessed using GPT-4. This resulted in Llama 2-Chat still showing a win-rate of over 60% against ChatGPT, although the advantage was less pronounced.\\n\\nHuman Evaluation\\n\\nHuman evaluations are often considered the gold standard for evaluating dialogue models, and the researchers used this method to assess the Llama 2-Chat models’ helpfulness and safety. The models were compared to open-source models like Falcon and MPT MosaicML as well as closed-source models like ChatGPT and PaLM using over 4,000 single and multi-turn prompts.\\n\\nThe results showed that Llama 2-Chat models significantly outperformed open-source models on both single turn and multi-turn prompts, with the Llama 2-Chat 34B model winning over 75% against comparably sized models. The largest Llama 2-Chat model was also competitive with ChatGPT.\\n\\nThree different annotators independently evaluated each model generation comparison to ensure inter-rater reliability (IRR), which was measured using Gwet’s AC1/2 statistic. Depending on the model comparison, the AC2 score varied between 0.37 and 0.55.\\n\\nHowever, the authors acknowledge that human evaluations have certain limitations. For instance, while the 4,000 prompt set is large by research standards, it doesn’t cover all possible real-world usage scenarios. The prompt set lacked diversity and didn’t include any coding- or reasoning-related prompts. Evaluations focused on the final generation of a multi-turn conversation, not the entire conversation experience. Finally, the subjective and noisy nature of human evaluations means results could vary with different prompts or instructions.\\n\\nSafety\\n\\nSafety in Pretraining\\n\\nThe authors discuss the pretraining data used for the Llama 2-Chat model and the steps taken to pretrain it responsibly. They didn’t use any user data and excluded certain sites that contain large amounts of personal information. They also aimed to minimize their carbon footprint and avoided additional filtering that could result in demographic erasure. However, the authors warn the model should be deployed only after significant safety tuning.\\n\\nThe demographic representation in the training data was analyzed, revealing an overrepresentation of “he” pronouns compared to “she” pronouns, which might lead to more frequent usage of “he” in the model’s outputs. The top demographic identity terms related to religion, gender, nationality, race and ethnicity, and sexual orientation all showed a Western skew.\\n\\nThe authors found a small amount of toxicity in the pretraining data, which may affect the output of the model. They also identified English as the dominant language in the training data, suggesting the model might not work as effectively with other languages.\\n\\nThe safety capabilities of Llama 2 were tested using three automatic benchmarks: TruthfulQA for truthfulness, ToxiGen for toxicity, and BOLD for bias. Compared to its predecessor, Llama 2 demonstrated increased truthfulness and decreased toxicity. However, the 13B and 70B versions of Llama 2 exhibited increased toxicity, potentially due to larger pretraining data or different dataset mixes. While the authors noted an increase in positive sentiment for many demographic groups, they emphasized the need for additional safety mitigations before deployment and more comprehensive studies of the model’s real-world impact.\\n\\nSafety Fine-Tuning\\n\\nThe authors discuss fine-tuning approach of a language model called Llama 2-Chat, outlining its techniques, safety categories, annotation guidelines, and methods to mitigate safety risks.\\n\\nSupervised Safety Fine-Tuning: Here, the team starts with adversarial prompts and safe demonstrations, included in the general supervised fine-tuning process. This helps align the model with safety guidelines early on.\\n\\nSafety RLHF (Reinforcement Learning from Human Feedback): This method integrates safety into the general RLHF pipeline, which involves training a safety-specific reward model and gathering more adversarial prompts for better fine-tuning.\\n\\nSafety Context Distillation: In this step, the model is refined by generating safer responses and distilling the safety context into the model. This is done using a targeted approach to choose if context distillation should be used for each sample.\\n\\nSafety categories identified are illicit and criminal activities, hateful and harmful activities, and unqualified advice. To cover different varieties of prompts, they use risk categories and attack vectors, such as psychological manipulation, logic manipulation, syntactic manipulation, semantic manipulation, and others.\\n\\nFor fine-tuning, prompts and demonstrations of safe model responses are gathered and used following the established guidelines. The model’s ability to write nuanced responses improves through RLHF.\\n\\nThe research team also found that adding an additional stage of safety mitigation does not negatively impact model performance on helpfulness. However, with more safety data mixed in model tuning, the model does answer certain questions in a more conservative manner, leading to an increase in the rate of false refusals (where the model refuses to answer legitimate prompts due to irrelevant safety concerns).\\n\\nLastly, context distillation is used to encourage the model to associate adversarial prompts with safer responses, and this context distillation only occurs on adversarial prompts to prevent the degradation of model performance. The safety reward model decides whether to use safety context distillation or not.\\n\\nRed Teaming\\n\\nThe researches discusses the application of red teaming as a proactive method of identifying potential risks and vulnerabilities in Language Learning Models (LLMs). These efforts involve more than 350 professionals from diverse fields such as cybersecurity, election fraud, legal, civil rights, software engineering, machine learning, and creative writing. The red teaming exercises focused on various risk categories, like criminal planning, human trafficking, privacy violations, etc., as well as different attack vectors. Some findings indicated that early models often failed to recognize and handle problematic content appropriately, but iterative improvements helped mitigate these issues.\\n\\nPost-exercise, the data collected was analyzed thoroughly, considering factors like dialogue length, risk area distribution, and the degree of risk. This information was used for model fine-tuning and safety training. The effectiveness of these red teaming exercises was measured using a robustness factor, defined as the average number of prompts that would trigger a violating response from the model per person per hour. For instance, on a 7B model, the robustness improved significantly over several red teaming iterations and model refinements.\\n\\nThe red teaming efforts continue to be a valuable tool in improving model safety and robustness, with new candidate releases consistently reducing the rate of prompts triggering violating responses. As a result, on average, there was a 90% rejection rate model over model.\\n\\nSafety Evaluation of Llama 2-Chat\\n\\nThe authors used human evaluation method to assess the safety of Language Learning Models (LLMs), specifically involving around 2,000 adversarial prompts. The responses to these prompts were assessed by raters on a five-point Likert scale, with 5 being the safest and most helpful, and 1 indicating severe safety violations. A rating of 1 or 2 was considered as a violation.\\n\\nThe violation percentage served as the primary evaluation metric, with mean rating as supplementary. Three annotators assessed each example, with a majority vote determining if a response was violating safety guidelines. Inter-rater reliability (IRR), measured using Gwet’s AC1/2 statistic, indicated a high degree of agreement among annotators. The IRR scores varied depending on the model being evaluated.\\n\\nThe overall violation percentage and safety rating of various LLMs showed that Llama 2-Chat performed comparably or better than others. It is important to note that the evaluations are influenced by factors such as prompt set limitations, review guidelines’ subjectivity, content standards, and individual raters’ subjectivity.\\n\\nThere was a trend observed that multi-turn conversations were more likely to induce unsafe responses across all models. However, Llama 2-Chat still performed well, particularly in multi-turn conversations.\\n\\nIn terms of truthfulness, toxicity, and bias, fine-tuned Llama 2-Chat showed great improvements over the pre-trained model. It showed the lowest level of toxicity among all compared models. Moreover, Llama 2-Chat showed increased positive sentiment for many demographic groups after fine-tuning. In-depth analyses and results of truthfulness and bias were provided in the appendix.\\n\\nDiscussions\\n\\nLearnings and Observations\\n\\nThe findings suggest that reinforcement learning was particularly effective in the tuning process due to its cost and time efficiency. The success of RLHF (Reinforcement Learning from Human Feedback) hinges on the synergistic relationship it creates between humans and LLMs during the annotation process. Notably, RLHF helps overcome the limitations of supervised fine-tuning and can lead to superior writing abilities in LLMs.\\n\\nAn interesting phenomenon related to RLHF was observed — dynamic re-scaling of temperature contingent upon the context. For creative prompts, increased temperature continues to generate diversity across RLHF iterations. However, for factual prompts, despite the rising temperature, the model learns to provide consistent responses.\\n\\nThe Llama 2-Chat model also demonstrated robust temporal organization abilities, which suggests LLMs might have a more advanced concept of time than previously thought.\\n\\nAn intriguing finding is the emergence of tool use in LLMs, which emerged spontaneously in a zero-shot context. Even though tool-use was not explicitly annotated, the model demonstrated the capability to utilize a sequence of tools in a zero-shot context. While promising, LLM tool use can also pose safety concerns and requires further research and testing.\\n\\nPaper Review\\n\\nDeep Learning\\n\\nNLP\\n\\nLarge Language Models\\n\\nOpen Source\\n\\n--\\n\\n--\\n\\nGoPenAI\\n\\nGoPenAI\\n\\nPublished in GoPenAI\\n\\n2.6K followers\\n\\nLast published 4 days ago\\n\\nWhere the ChatGPT community comes together to share insights and stories.\\n\\nAndrew Lukyanenko\\n\\nAndrew Lukyanenko\\n\\nWritten by Andrew Lukyanenko\\n\\n2.7K followers\\n\\n36 following\\n\\nEconomist by education. Polyglot as a hobby. DS as a calling. https://andlukyane.com/\\n\\nNo responses yet\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nRules\\n\\nTerms\\n\\nText to speech')]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-GHqTPkS2pk",
        "outputId": "18011f7c-df19-45f9-8ce3-a204bfa6b3b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UYLOmDoTASC"
      },
      "source": [
        "#Split the Text into chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "laq6MWjaS-on"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(separator='\\n',\n",
        "                                      chunk_size = 1000,\n",
        "                                      chunk_overlap = 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dl1l62PNTSRE"
      },
      "outputs": [],
      "source": [
        "text_chunks = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH18ZdcCTaJ_",
        "outputId": "1e2b76b6-c323-43f8-a53e-adbf633f956c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JkRFfj_Tc05",
        "outputId": "6008bd09-3468-499f-d8e8-281fa4fe9ae5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://stability.ai/news/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models'}, page_content='Stability AI Launches the First of its Stable LM Suite of Language Models\\nProduct\\nApr 19\\nWritten By Joshua Lopez\\nToday, Stability AI released a new open source language model, Stable LM. The Alpha version of the model is available in 3 billion and 7 billion parameters, with 15 billion to 65 billion parameter models to follow. Developers can freely inspect, use, and adapt our Stable LM base models for commercial or research purposes, subject to the terms of the CC BY-SA-4.0 license.\\nIn 2022, Stability AI drove the public release of Stable Diffusion, a revolutionary image model representing a transparent, open, and scalable alternative to proprietary AI. With the launch of the Stable LM suite of models, Stability AI is continuing to make foundational AI technology accessible to all. Our Stable LM models can generate text and code and will power various downstream applications. They demonstrate how small and efficient models can deliver high performance with appropriate training.')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_chunks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJaLBUkuT7rw",
        "outputId": "243b725d-7d40-4ac7-faeb-47ce29a4cb6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk 1:\n",
            "page_content='Stability AI Launches the First of its Stable LM Suite of Language Models\n",
            "Product\n",
            "Apr 19\n",
            "Written By Joshua Lopez\n",
            "Today, Stability AI released a new open source language model, Stable LM. The Alpha version of the model is available in 3 billion and 7 billion parameters, with 15 billion to 65 billion parameter models to follow. Developers can freely inspect, use, and adapt our Stable LM base models for commercial or research purposes, subject to the terms of the CC BY-SA-4.0 license.\n",
            "In 2022, Stability AI drove the public release of Stable Diffusion, a revolutionary image model representing a transparent, open, and scalable alternative to proprietary AI. With the launch of the Stable LM suite of models, Stability AI is continuing to make foundational AI technology accessible to all. Our Stable LM models can generate text and code and will power various downstream applications. They demonstrate how small and efficient models can deliver high performance with appropriate training.' metadata={'source': 'https://stability.ai/news/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models'}\n",
            "---\n",
            "Chunk 2:\n",
            "page_content='The release of Stable LM builds on our experience in open sourcing earlier language models with EleutherAI, a nonprofit research hub. These language models include GPT-J, GPT-NeoX, and the Pythia suite, trained on The Pile open source dataset. Many recent open source language models continue to build on these efforts, including Cerebras-GPT and Dolly-2.\n",
            "Stable LM is trained on a new experimental dataset built on The Pile, but three times larger with 1.5 trillion content tokens. We will release details on the dataset in due course. The richness of this dataset gives Stable LM surprisingly high performance in conversational and coding tasks despite its small size of 3 to 7 billion parameters (by comparison, GPT-3 has 175 billion parameters).' metadata={'source': 'https://stability.ai/news/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models'}\n",
            "---\n",
            "Chunk 3:\n",
            "page_content='We are also releasing a set of research models that are fine-tuned instructions. Initially, these fine-tuned models will use a combination of five recent open source datasets for conversational agents: Alpaca, GPT4All, Dolly, ShareGPT, and HH. These fine-tuned models are intended for research use only and are released under a noncommercial CC BY-NC-SA 4.0 license, in line with Stanford’s Alpaca license.\n",
            "Check out some examples below, produced by our 7 billion parameters fine-tuned model:\n",
            "View fullsize\n",
            "example 1.png\n",
            "View fullsize\n",
            "example 2.png\n",
            "View fullsize\n",
            "example 3.png\n",
            "Language models will form the backbone of our digital economy, and we want everyone to have a voice in their design. Models like Stable LM demonstrate our commitment to AI technology that is transparent, accessible, and supportive:' metadata={'source': 'https://stability.ai/news/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models'}\n",
            "---\n",
            "Chunk 4:\n",
            "page_content='Transparent. We open source our models to promote transparency and foster trust. Researchers can “look under the hood” to verify performance, work on interpretability techniques, identify potential risks, and help develop safeguards. Organizations across the public and private sectors can adapt (“fine-tune”) these open source models for their own applications without sharing their sensitive data or giving up control of their AI capabilities.\n",
            "Accessible. We design for the edge so everyday users can run our models on local devices. Using these models, developers can build independent applications compatible with widely available hardware instead of relying on proprietary services from one or two companies. In this way, the economic benefits of AI are shared by a broad community of users and developers. Open, fine-grained access to our models allows the broad research and academic community to develop interpretability and safety techniques beyond what is possible with closed models.' metadata={'source': 'https://stability.ai/news/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models'}\n",
            "---\n",
            "Chunk 5:\n",
            "page_content='Supportive. We build models to support our users, not replace them. We are focused on efficient, specialized, and practical AI performance – not a quest for god-like intelligence. We develop tools that help everyday people and everyday firms use AI to unlock creativity, boost their productivity, and open up new economic opportunities.\n",
            "The models are now available in our GitHub repository. We will publish a full technical report in the near future and look forward to ongoing collaboration with developers and researchers as we roll out the Stable LM suite. In addition, we will be kicking off our crowd-sourced RLHF program and working with community efforts such as Open Assistant to create an open source dataset for AI assistants.\n",
            "We will be releasing more models soon and are growing our team. If you are passionate about democratizing access to this technology and experienced in LLMs, please apply here!\n",
            "Joshua Lopez\n",
            "Previous\n",
            "Previous\n",
            "Stability AI releases its Image Upscaling API\n",
            "Next\n",
            "Next' metadata={'source': 'https://stability.ai/news/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models'}\n",
            "---\n",
            "Chunk 6:\n",
            "page_content='Joshua Lopez\n",
            "Previous\n",
            "Previous\n",
            "Stability AI releases its Image Upscaling API\n",
            "Next\n",
            "Next\n",
            "Stability AI Partners with Iconic Artist Peter Gabriel to Launch Series of AI Animation Challenges titled #DiffuseTogether' metadata={'source': 'https://stability.ai/news/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models'}\n",
            "---\n",
            "Chunk 7:\n",
            "page_content='Sitemap\n",
            "Open in app\n",
            "Sign up\n",
            "Sign in\n",
            "Write\n",
            "Sign up\n",
            "Sign in\n",
            "GoPenAI\n",
            "Follow publication\n",
            "GoPenAI\n",
            "Where the ChatGPT community comes together to share insights and stories.\n",
            "Follow publication\n",
            "Paper Review\n",
            "Paper Review: Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
            "Llama 2: one of the best open source models\n",
            "Andrew Lukyanenko\n",
            "Andrew Lukyanenko\n",
            "15 min read\n",
            "Jul 20, 2023\n",
            "--\n",
            "Project link\n",
            "Model link\n",
            "Paper link' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 8:\n",
            "page_content='Llama 2: one of the best open source models\n",
            "Andrew Lukyanenko\n",
            "Andrew Lukyanenko\n",
            "15 min read\n",
            "Jul 20, 2023\n",
            "--\n",
            "Project link\n",
            "Model link\n",
            "Paper link\n",
            "The authors of the work present Llama 2, an assortment of pretrained and fine-tuned large language models (LLMs) with sizes varying from 7 billion to 70 billion parameters. The fine-tuned versions, named Llama 2-Chat, are specifically designed for dialogue applications. These models surpass the performance of existing open-source chat models on most benchmarks, and according to human evaluations for usefulness and safety, they could potentially replace closed-source models. The authors also detail their approach to fine-tuning and safety enhancements for Llama 2-Chat to support the community in further developing and responsibly handling LLMs.\n",
            "Pretraining' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 9:\n",
            "page_content='Pretraining\n",
            "The authors developed the Llama 2 model family starting from the pretraining methodology of Llama, which utilizes an optimized auto-regressive transformer. They implemented several modifications for improved performance, including enhanced data cleaning, updated data mixes, training on 40% more total tokens, and doubling the context length. They also incorporated grouped-query attention (GQA) to enhance the inference scalability for their larger models.\n",
            "Pretraining Data' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 10:\n",
            "page_content='Pretraining Data\n",
            "The authors utilized a novel mix of data from publicly accessible sources to train the Llama 2 models, excluding any data from Meta’s products or services. They made efforts to erase data from certain sites known for harboring large amounts of personal information about private individuals. They trained the models on 2 trillion tokens of data, believing this amount provided a beneficial performance-cost balance. They also up-sampled the most factual sources to boost knowledge and reduce instances of false information generation or “hallucinations”.\n",
            "Llama 2 Pretrained Model Evaluation\n",
            "Llama 2 models significantly outperform their Llama 1 counterparts:\n",
            "The 70 billion-parameter Llama 2 model notably improves results on the MMLU and BBH benchmarks by roughly 5 and 8 points, respectively, when compared to the 65 billion-parameter Llama 1 model.\n",
            "Llama 2 models with 7 billion and 30 billion parameters outdo MPT models of similar size in all categories except code benchmarks.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 11:\n",
            "page_content='Llama 2 models with 7 billion and 30 billion parameters outdo MPT models of similar size in all categories except code benchmarks.\n",
            "In comparison with Falcon models, Llama 2’s 7 billion and 34 billion parameter models outperform the 7 billion and 40 billion parameter Falcon models in all benchmark categories.\n",
            "Moreover, the Llama 2 70B model surpasses all open-source models.\n",
            "Comparatively, the Llama 2 70B model performs similarly to the closed-source GPT-3.5 (OpenAI, 2023) on the MMLU and GSM8K benchmarks but shows a significant deficit on coding benchmarks. It matches or exceeds the performance of PaLM (540 billion parameters) on nearly all benchmarks. However, there remains a substantial performance gap between the Llama 2 70B model and both GPT-4 and PaLM-2-L.\n",
            "Fine-tuning\n",
            "Supervised Fine-Tuning (SFT)' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 12:\n",
            "page_content='Fine-tuning\n",
            "Supervised Fine-Tuning (SFT)\n",
            "The authors initiated the Supervised Fine-Tuning (SFT) phase using publicly available instruction tuning data like in Llama. However, they observed that many third-party SFT data sources lacked diversity and quality, particularly for aligning Large Language Models (LLMs) towards dialogue-style instructions. Therefore, they prioritized collecting several thousand high-quality SFT data examples, and found that using fewer but better-quality examples led to notable performance improvements.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 13:\n",
            "page_content='The authors discovered that tens of thousands of SFT annotations were enough to achieve high-quality results, and ceased after collecting 27,540 annotations. They highlighted the significant impact of different annotation platforms and vendors on model performance, emphasizing the need for data checks even when sourcing annotations from vendors. A manual examination of a set of 180 examples showed that model outputs were often competitive with those handwritten by human annotators, suggesting the value in shifting more annotation efforts to preference-based annotation for Reinforcement Learning from Human Feedback (RLHF).\n",
            "In fine-tuning, each sample consisted of a prompt and an answer, concatenated together with a special token used to separate the segments. The authors used an autoregressive objective and zeroed-out the loss on tokens from the user prompt, meaning they only backpropagated on answer tokens.\n",
            "Human Preference Data Collection' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 14:\n",
            "page_content='Human Preference Data Collection\n",
            "The authors used a binary comparison protocol to collect human preference data for reward modeling in order to maximize the diversity of prompts. Annotators were tasked with writing a prompt and choosing between two model responses based on set criteria. These responses were sampled from two different model variants and varied by temperature hyperparameter. Annotators also had to rate their preference for the chosen response over the alternative (significantly better, better, slightly better, or negligibly better/ unsure.).\n",
            "The focus of these preference annotations was on “helpfulness” and “safety”. The authors define “helpfulness” as how well Llama 2-Chat responses fulfil users’ requests, and “safety” as whether responses comply with their safety guidelines. Separate guidelines were provided for each focus area.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 15:\n",
            "page_content='During the safety stage, model responses were categorized into three groups: 1) the preferred response is safe and the other is not, 2) both responses are safe, and 3) both responses are unsafe. No examples where the chosen response was unsafe and the other safe were included.\n",
            "Human annotations were collected in weekly batches. As more preference data was collected, their reward models improved, allowing the authors to train progressively better versions of Llama 2-Chat. As improvements shifted the model’s data distribution, the authors collected new preference data using the latest Llama 2-Chat iterations to keep the reward model up-to-date and accurate.\n",
            "The authors collected over 1 million binary comparisons, referred to as Meta reward modeling data. Compared to existing open-source datasets, their preference data has more conversation turns and is, on average, longer.\n",
            "Reward Modeling' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 16:\n",
            "page_content='Reward Modeling\n",
            "The authors developed a reward model that inputs a model response and corresponding prompt and outputs a score indicating the quality (e.g., helpfulness, safety) of the generated response. These scores can then be used as rewards to optimize the Llama 2-Chat model for better alignment with human preferences.\n",
            "They trained two separate reward models: one optimized for helpfulness (Helpfulness RM) and another for safety (Safety RM). These models were initialized from pre-trained chat model checkpoints, ensuring knowledge transfer and preventing discrepancies such as favoring hallucinations.\n",
            "To train the reward model, pairwise human preference data was converted into a binary ranking label format. The model was trained to ensure that the chosen response scored higher than its counterpart. To leverage the four-point preference rating scale, they added a margin component to the loss to help the model assign more distinct scores to responses that have more differences.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 17:\n",
            "page_content='The authors combined their new data with existing open-source preference datasets to form a larger training dataset.\n",
            "Reward Model Results\n",
            "The authors evaluated their reward models on a test set held out from each batch of human preference annotation data. They compared their models with publicly available alternatives, including SteamSHP-XL, the Open Assistant reward model, and GPT-4. Their models performed the best, particularly on the corresponding internal test sets.\n",
            "The authors noted a tension between the goals of helpfulness and safety, and suggested this might be why their separate models performed best on their own domain. A single model that aims to perform well on both aspects would need to differentiate between better responses and distinguish safe prompts from adversarial ones.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 18:\n",
            "page_content='When scoring by preference rating, accuracy was superior for “significantly better” test sets, and it degraded as comparisons became more similar. The authors pointed out that accuracy on more distinct responses is key to improving Llama 2-Chat’s performance.\n",
            "In terms of scaling trends, the authors found that larger models provided better performance for similar volumes of data, and performance had not plateaued with the current volume of annotated data. The authors concluded that improving the reward model’s accuracy could directly improve Llama 2-Chat’s performance, as the ranking task of the reward is unambiguous.\n",
            "Iterative Fine-Tuning' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 19:\n",
            "page_content='Iterative Fine-Tuning\n",
            "Two main algorithms were used for RLHF fine-tuning: Proximal Policy Optimization (PPO), standard in RLHF literature, and Rejection Sampling fine-tuning, where they selected the best output from sampled model responses for a gradient update. The differences between the two algorithms lie in breadth (K samples for a given prompt in Rejection Sampling) and depth (in PPO the sample is a function of the updated model policy from the previous step, in RS all outputs are sampled. In iterative training the differences are less pronounced).\n",
            "Rejection Sampling fine-tuning was performed only with the largest 70B Llama 2-Chat, with smaller models fine-tuned on the sampled data from the larger model. Over iterations, the authors adjusted their strategy to include top-performing samples from all prior iterations, leading to significant performance improvements.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 20:\n",
            "page_content='The authors illustrate the benefit of Rejection Sampling in two ways. They show the delta between the maximum and median curves can be interpreted as the potential gain of fine-tuning on the best output. They also found that the optimal temperature for generating diverse samples isn’t constant during iterative model updates.\n",
            "After RLHF (V4), the authors sequentially combined Rejection Sampling and PPO fine-tuning. For PPO, they iteratively improved the policy by sampling prompts and generations from the policy and used the PPO algorithm to achieve the objective. They also added a penalty term for diverging from the original policy, as it’s helpful for training stability and to reduce reward hacking.\n",
            "System Message for Multi-Turn Consistency' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 21:\n",
            "page_content='System Message for Multi-Turn Consistency\n",
            "The authors proposed Ghost Attention (GAtt), a technique designed to help AI remember initial instructions throughout a dialogue. This method, which builds on the concept of Context Distillation, introduces an instruction that needs to be followed throughout the conversation and is appended to all user messages in a synthetic dialogue dataset. During training, the instruction is only kept in the first turn and the loss is set to zero for all tokens from prior turns. This strategy was applied to a range of synthetic constraints including hobbies, language, and public figures. The implementation of GAtt helped maintain attention towards initial instructions over a larger part of the dialogue.\n",
            "GAtt managed to ensure consistency even over 20+ turns, until the maximum context length was reached. While this initial implementation has been beneficial, the authors believe there is potential for further enhancement and iteration on this technique.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 22:\n",
            "page_content='RLHF Results\n",
            "Model-Based Evaluation\n",
            "Evaluating large language models (LLMs) like Llama 2-Chat is a complex problem. While human evaluation is considered the gold standard, it is not always scalable and may present complications. As a solution, the authors first used reward models to measure improvement in iterations of their Reinforcement Learning from Human Feedback (RLHF) model versions, and later confirmed these findings with human evaluations.\n",
            "To test the reward model’s reliability, the authors collected a test set of prompts and had them judged by human annotators. The results indicated that the reward models were generally well-aligned with human preferences, validating their use as a point-wise metric.\n",
            "However, to prevent possible divergence from human preferences, the authors also utilized a more general reward model trained on diverse open-source Reward Modeling datasets. They hypothesize that iterative model updates may help maintain alignment with human preferences.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 23:\n",
            "page_content='In a final check to ensure no regression between new and old models, both models were used in the next annotation iteration for comparison.\n",
            "The authors’ models were shown to outperform ChatGPT in both safety and helpfulness after RLHF-V3. For fair comparison, final results were also assessed using GPT-4. This resulted in Llama 2-Chat still showing a win-rate of over 60% against ChatGPT, although the advantage was less pronounced.\n",
            "Human Evaluation\n",
            "Human evaluations are often considered the gold standard for evaluating dialogue models, and the researchers used this method to assess the Llama 2-Chat models’ helpfulness and safety. The models were compared to open-source models like Falcon and MPT MosaicML as well as closed-source models like ChatGPT and PaLM using over 4,000 single and multi-turn prompts.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 24:\n",
            "page_content='The results showed that Llama 2-Chat models significantly outperformed open-source models on both single turn and multi-turn prompts, with the Llama 2-Chat 34B model winning over 75% against comparably sized models. The largest Llama 2-Chat model was also competitive with ChatGPT.\n",
            "Three different annotators independently evaluated each model generation comparison to ensure inter-rater reliability (IRR), which was measured using Gwet’s AC1/2 statistic. Depending on the model comparison, the AC2 score varied between 0.37 and 0.55.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 25:\n",
            "page_content='However, the authors acknowledge that human evaluations have certain limitations. For instance, while the 4,000 prompt set is large by research standards, it doesn’t cover all possible real-world usage scenarios. The prompt set lacked diversity and didn’t include any coding- or reasoning-related prompts. Evaluations focused on the final generation of a multi-turn conversation, not the entire conversation experience. Finally, the subjective and noisy nature of human evaluations means results could vary with different prompts or instructions.\n",
            "Safety\n",
            "Safety in Pretraining' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 26:\n",
            "page_content='Safety\n",
            "Safety in Pretraining\n",
            "The authors discuss the pretraining data used for the Llama 2-Chat model and the steps taken to pretrain it responsibly. They didn’t use any user data and excluded certain sites that contain large amounts of personal information. They also aimed to minimize their carbon footprint and avoided additional filtering that could result in demographic erasure. However, the authors warn the model should be deployed only after significant safety tuning.\n",
            "The demographic representation in the training data was analyzed, revealing an overrepresentation of “he” pronouns compared to “she” pronouns, which might lead to more frequent usage of “he” in the model’s outputs. The top demographic identity terms related to religion, gender, nationality, race and ethnicity, and sexual orientation all showed a Western skew.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 27:\n",
            "page_content='The authors found a small amount of toxicity in the pretraining data, which may affect the output of the model. They also identified English as the dominant language in the training data, suggesting the model might not work as effectively with other languages.\n",
            "The safety capabilities of Llama 2 were tested using three automatic benchmarks: TruthfulQA for truthfulness, ToxiGen for toxicity, and BOLD for bias. Compared to its predecessor, Llama 2 demonstrated increased truthfulness and decreased toxicity. However, the 13B and 70B versions of Llama 2 exhibited increased toxicity, potentially due to larger pretraining data or different dataset mixes. While the authors noted an increase in positive sentiment for many demographic groups, they emphasized the need for additional safety mitigations before deployment and more comprehensive studies of the model’s real-world impact.\n",
            "Safety Fine-Tuning' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 28:\n",
            "page_content='Safety Fine-Tuning\n",
            "The authors discuss fine-tuning approach of a language model called Llama 2-Chat, outlining its techniques, safety categories, annotation guidelines, and methods to mitigate safety risks.\n",
            "Supervised Safety Fine-Tuning: Here, the team starts with adversarial prompts and safe demonstrations, included in the general supervised fine-tuning process. This helps align the model with safety guidelines early on.\n",
            "Safety RLHF (Reinforcement Learning from Human Feedback): This method integrates safety into the general RLHF pipeline, which involves training a safety-specific reward model and gathering more adversarial prompts for better fine-tuning.\n",
            "Safety Context Distillation: In this step, the model is refined by generating safer responses and distilling the safety context into the model. This is done using a targeted approach to choose if context distillation should be used for each sample.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 29:\n",
            "page_content='Safety categories identified are illicit and criminal activities, hateful and harmful activities, and unqualified advice. To cover different varieties of prompts, they use risk categories and attack vectors, such as psychological manipulation, logic manipulation, syntactic manipulation, semantic manipulation, and others.\n",
            "For fine-tuning, prompts and demonstrations of safe model responses are gathered and used following the established guidelines. The model’s ability to write nuanced responses improves through RLHF.\n",
            "The research team also found that adding an additional stage of safety mitigation does not negatively impact model performance on helpfulness. However, with more safety data mixed in model tuning, the model does answer certain questions in a more conservative manner, leading to an increase in the rate of false refusals (where the model refuses to answer legitimate prompts due to irrelevant safety concerns).' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 30:\n",
            "page_content='Lastly, context distillation is used to encourage the model to associate adversarial prompts with safer responses, and this context distillation only occurs on adversarial prompts to prevent the degradation of model performance. The safety reward model decides whether to use safety context distillation or not.\n",
            "Red Teaming' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 31:\n",
            "page_content='Red Teaming\n",
            "The researches discusses the application of red teaming as a proactive method of identifying potential risks and vulnerabilities in Language Learning Models (LLMs). These efforts involve more than 350 professionals from diverse fields such as cybersecurity, election fraud, legal, civil rights, software engineering, machine learning, and creative writing. The red teaming exercises focused on various risk categories, like criminal planning, human trafficking, privacy violations, etc., as well as different attack vectors. Some findings indicated that early models often failed to recognize and handle problematic content appropriately, but iterative improvements helped mitigate these issues.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 32:\n",
            "page_content='Post-exercise, the data collected was analyzed thoroughly, considering factors like dialogue length, risk area distribution, and the degree of risk. This information was used for model fine-tuning and safety training. The effectiveness of these red teaming exercises was measured using a robustness factor, defined as the average number of prompts that would trigger a violating response from the model per person per hour. For instance, on a 7B model, the robustness improved significantly over several red teaming iterations and model refinements.\n",
            "The red teaming efforts continue to be a valuable tool in improving model safety and robustness, with new candidate releases consistently reducing the rate of prompts triggering violating responses. As a result, on average, there was a 90% rejection rate model over model.\n",
            "Safety Evaluation of Llama 2-Chat' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 33:\n",
            "page_content='Safety Evaluation of Llama 2-Chat\n",
            "The authors used human evaluation method to assess the safety of Language Learning Models (LLMs), specifically involving around 2,000 adversarial prompts. The responses to these prompts were assessed by raters on a five-point Likert scale, with 5 being the safest and most helpful, and 1 indicating severe safety violations. A rating of 1 or 2 was considered as a violation.\n",
            "The violation percentage served as the primary evaluation metric, with mean rating as supplementary. Three annotators assessed each example, with a majority vote determining if a response was violating safety guidelines. Inter-rater reliability (IRR), measured using Gwet’s AC1/2 statistic, indicated a high degree of agreement among annotators. The IRR scores varied depending on the model being evaluated.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 34:\n",
            "page_content='The overall violation percentage and safety rating of various LLMs showed that Llama 2-Chat performed comparably or better than others. It is important to note that the evaluations are influenced by factors such as prompt set limitations, review guidelines’ subjectivity, content standards, and individual raters’ subjectivity.\n",
            "There was a trend observed that multi-turn conversations were more likely to induce unsafe responses across all models. However, Llama 2-Chat still performed well, particularly in multi-turn conversations.\n",
            "In terms of truthfulness, toxicity, and bias, fine-tuned Llama 2-Chat showed great improvements over the pre-trained model. It showed the lowest level of toxicity among all compared models. Moreover, Llama 2-Chat showed increased positive sentiment for many demographic groups after fine-tuning. In-depth analyses and results of truthfulness and bias were provided in the appendix.\n",
            "Discussions\n",
            "Learnings and Observations' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 35:\n",
            "page_content='Discussions\n",
            "Learnings and Observations\n",
            "The findings suggest that reinforcement learning was particularly effective in the tuning process due to its cost and time efficiency. The success of RLHF (Reinforcement Learning from Human Feedback) hinges on the synergistic relationship it creates between humans and LLMs during the annotation process. Notably, RLHF helps overcome the limitations of supervised fine-tuning and can lead to superior writing abilities in LLMs.\n",
            "An interesting phenomenon related to RLHF was observed — dynamic re-scaling of temperature contingent upon the context. For creative prompts, increased temperature continues to generate diversity across RLHF iterations. However, for factual prompts, despite the rising temperature, the model learns to provide consistent responses.\n",
            "The Llama 2-Chat model also demonstrated robust temporal organization abilities, which suggests LLMs might have a more advanced concept of time than previously thought.' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 36:\n",
            "page_content='The Llama 2-Chat model also demonstrated robust temporal organization abilities, which suggests LLMs might have a more advanced concept of time than previously thought.\n",
            "An intriguing finding is the emergence of tool use in LLMs, which emerged spontaneously in a zero-shot context. Even though tool-use was not explicitly annotated, the model demonstrated the capability to utilize a sequence of tools in a zero-shot context. While promising, LLM tool use can also pose safety concerns and requires further research and testing.\n",
            "Paper Review\n",
            "Deep Learning\n",
            "NLP\n",
            "Large Language Models\n",
            "Open Source\n",
            "--\n",
            "--\n",
            "GoPenAI\n",
            "GoPenAI\n",
            "Published in GoPenAI\n",
            "2.6K followers\n",
            "Last published 4 days ago\n",
            "Where the ChatGPT community comes together to share insights and stories.\n",
            "Andrew Lukyanenko\n",
            "Andrew Lukyanenko\n",
            "Written by Andrew Lukyanenko\n",
            "2.7K followers\n",
            "36 following\n",
            "Economist by education. Polyglot as a hobby. DS as a calling. https://andlukyane.com/\n",
            "No responses yet\n",
            "Help\n",
            "Status\n",
            "About\n",
            "Careers\n",
            "Press\n",
            "Blog\n",
            "Privacy\n",
            "Rules' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n",
            "Chunk 37:\n",
            "page_content='2.7K followers\n",
            "36 following\n",
            "Economist by education. Polyglot as a hobby. DS as a calling. https://andlukyane.com/\n",
            "No responses yet\n",
            "Help\n",
            "Status\n",
            "About\n",
            "Careers\n",
            "Press\n",
            "Blog\n",
            "Privacy\n",
            "Rules\n",
            "Terms\n",
            "Text to speech' metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "for i, chunk in enumerate(text_chunks):\n",
        "    print(f\"Chunk {i+1}:\\n{chunk}\\n---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSSoODZGUBHD"
      },
      "source": [
        "#Download the OPENAI Embeddings or Hugging Face Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jK0KYcyHT8Z7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\yogit\\AppData\\Local\\Temp\\ipykernel_26132\\2174154411.py:11: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings()\n"
          ]
        }
      ],
      "source": [
        "# model_name = \"BAAI/bge-large-en\"\n",
        "# model_kwargs = {'device': 'cpu'}\n",
        "# encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "# embeddings = HuggingFaceBgeEmbeddings(\n",
        "#     model_name=model_name,\n",
        "#     model_kwargs=model_kwargs,\n",
        "#     encode_kwargs=encode_kwargs\n",
        "#)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbvVTMVJULr6",
        "outputId": "40a2bc68-2919-40d0-ec66-9d0d9b1ad4ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x0000023A47846CB0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x0000023A3F960580>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_JjBOvFcVGu",
        "outputId": "53a77fce-5a5d-4598-a1ee-c4d8f8807991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1536\n"
          ]
        }
      ],
      "source": [
        "query = embeddings.embed_query(\"Hello world\")\n",
        "print(len(query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46hkoDtoYWu_"
      },
      "source": [
        "#Convert the Text Chunks into Embeddings and Create a Knowledge Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "px20j8E7VF9H"
      },
      "outputs": [],
      "source": [
        "vectorstore=FAISS.from_documents(text_chunks, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvGASYLUcjmG"
      },
      "source": [
        "#Create a Large Language Model(LLM) Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eethhB6_ciC1",
        "outputId": "2c759627-354e-4480-83bf-66b7573c9928"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\yogit\\AppData\\Local\\Temp\\ipykernel_26132\\1815013380.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000023A48B12CB0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000023A48B131C0>, model_kwargs={}, openai_api_key='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', openai_proxy='')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm = ChatOpenAI()\n",
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "Q1IjocoScxEe",
        "outputId": "cf31b941-07e8-4ffc-f5f2-2fbf09bb70e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\yogit\\AppData\\Local\\Temp\\ipykernel_26132\\3887031814.py:1: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  llm.predict(\"What is AWS?\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Amazon Web Services (AWS) is a comprehensive, evolving cloud computing platform provided by Amazon.com. It offers a wide range of infrastructure services such as computing power, storage options, networking, databases, analytics, and machine learning, as well as various application services. AWS allows businesses to scale and grow their infrastructure without the need for large capital investments in physical servers and data centers. It is one of the leading cloud computing providers in the world.'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.predict(\"What is AWS?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7OJ7-Iici4"
      },
      "source": [
        "#Initialize the Retrieval QA with Source Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "LADW0So8i783"
      },
      "outputs": [],
      "source": [
        "chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I9DCijXjAZg",
        "outputId": "6933469e-8629-46cb-b7bd-2d0c04db6f8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\yogit\\AppData\\Local\\Temp\\ipykernel_26132\\482299641.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = chain({\"question\": \"Whait is Amazon Sagemaker?\"}, return_only_outputs=True)\n"
          ]
        }
      ],
      "source": [
        "result = chain({\"question\": \"What is Amazon Sagemaker?\"}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4LFMZ_K-jbpK",
        "outputId": "d62e0d12-acfb-4375-be57-3669908e9cc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly.\\n'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcmYKHfljisU",
        "outputId": "bb304be2-9abc-4e31-8f7a-2b38929bee67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly.\n"
          ]
        }
      ],
      "source": [
        "wrapped_text = textwrap.fill(result['answer'], width=500)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YqNSCEETjvHJ"
      },
      "outputs": [],
      "source": [
        "result=chain({\"question\": \"How does Llama 2 outperforms other models\"}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "AKCGQoLHj6mm",
        "outputId": "edd6c95a-27d3-4ee6-f2a2-c4fc08c57294"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Llama 2 outperforms other models by presenting an assortment of pretrained and fine-tuned large language models with sizes varying from 7 billion to 70 billion parameters. The fine-tuned versions, named Llama 2-Chat, are specifically designed for dialogue applications and surpass the performance of existing open-source chat models on most benchmarks.\\n'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSqRNk-9kCH5",
        "outputId": "cb715ec6-ec40-47be-881b-2a38697bfacd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Llama 2 outperforms other models by presenting an assortment of pretrained and fine-tuned large language models with sizes varying from 7 billion to 70 billion parameters. The fine-tuned versions, named Llama 2-Chat, are specifically designed for dialogue applications and surpass the performance of existing open-source chat models on most benchmarks.\n"
          ]
        }
      ],
      "source": [
        "wrapped_text = textwrap.fill(result['answer'], width=500)\n",
        "print(wrapped_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmdrkKIgkPSZ",
        "outputId": "9ae7111c-73ff-48da-f308-0cb0f0e37c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Amazon SageMaker is a fully managed service that provides developers and data scientists with the ability to build, train, and deploy machine learning models quickly.\n",
            "\n",
            "Answer: I don't know.\n",
            "\n",
            "Answer: Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly.\n",
            "\n",
            "Exiting\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "  query=input(f\"Prompt: \")\n",
        "  if query == 'exit':\n",
        "    print('Exiting')\n",
        "    break\n",
        "  if query =='':\n",
        "    continue\n",
        "  result=chain({'question':query})\n",
        "  print(f\"Answer: \" +result[\"answer\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYTT_PsMk3tu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
