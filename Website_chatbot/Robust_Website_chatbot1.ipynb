{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain langchain-community openai tiktoken faiss-cpu numpy nltk pypdf sentence-transformers transformers tokenizers unstructured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yogit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import os\n",
    "import sys\n",
    "import textwrap\n",
    "import getpass\n",
    "import nltk\n",
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully\n",
      "URLs: 3 configured\n",
      "Chunk size: 1000\n",
      "Embedding model: openai\n"
     ]
    }
   ],
   "source": [
    "class ChatbotConfig:\n",
    "    def __init__(self):\n",
    "        # Set OpenAI API Key securely\n",
    "        if 'OPENAI_API_KEY' not in os.environ:\n",
    "            api_key = getpass.getpass('Enter your OpenAI API Key: ')\n",
    "            if api_key.strip():\n",
    "                os.environ['OPENAI_API_KEY'] = api_key\n",
    "            else:\n",
    "                raise ValueError(\"API key is required\")\n",
    "        \n",
    "        # URLs to scrape\n",
    "        self.urls = [\n",
    "            'https://stability.ai/news/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models',\n",
    "            'https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html',\n",
    "            'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb',\n",
    "        ]\n",
    "        \n",
    "        # Text splitting parameters\n",
    "        self.chunk_size = 1000\n",
    "        self.chunk_overlap = 200\n",
    "        \n",
    "        # Model configuration\n",
    "        self.embedding_model = 'openai'  # 'openai' or 'huggingface'\n",
    "        self.llm_model = 'gpt-3.5-turbo'  # OpenAI model to use\n",
    "        \n",
    "        # Validation\n",
    "        self.validate()\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate configuration\"\"\"\n",
    "        if not self.urls:\n",
    "            raise ValueError(\"At least one URL is required\")\n",
    "        if self.chunk_size <= 0:\n",
    "            raise ValueError(\"Chunk size must be positive\")\n",
    "        if self.chunk_overlap >= self.chunk_size:\n",
    "            raise ValueError(\"Chunk overlap must be less than chunk size\")\n",
    "\n",
    "# Initialize configuration\n",
    "try:\n",
    "    config = ChatbotConfig()\n",
    "    print(\"Configuration loaded successfully\")\n",
    "    print(f\"URLs: {len(config.urls)} configured\")\n",
    "    print(f\"Chunk size: {config.chunk_size}\")\n",
    "    print(f\"Embedding model: {config.embedding_model}\")\n",
    "except Exception as e:\n",
    "    print(f\"Configuration error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Handles document loading and processing\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_documents(urls, max_retries=3):\n",
    "        \"\"\"Load documents from URLs \"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Loading documents from {len(urls)} URLs...\")\n",
    "                loader = UnstructuredURLLoader(urls=urls)\n",
    "                docs = loader.load()\n",
    "                \n",
    "                if not docs:\n",
    "                    print(\"No data loaded from URLs\")\n",
    "                    return []\n",
    "                \n",
    "                print(f\"Successfully loaded {len(docs)} documents\")\n",
    "                return docs\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    print(\"All retry attempts failed\")\n",
    "                    return []\n",
    "                print(\"Retrying...\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_documents(docs, chunk_size, chunk_overlap):\n",
    "        \"\"\"Split documents into chunks\"\"\"\n",
    "        try:\n",
    "            splitter = CharacterTextSplitter(\n",
    "                separator='\\n',\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap\n",
    "            )\n",
    "            chunks = splitter.split_documents(docs)\n",
    "            print(f\"Split documents into {len(chunks)} chunks\")\n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting documents: {e}\")\n",
    "            return []\n",
    "\n",
    "class EmbeddingManager:\n",
    "    \"\"\"Manages embedding models\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_embeddings(model_type='openai', **kwargs):\n",
    "        \"\"\"Get embedding model based on type\"\"\"\n",
    "        try:\n",
    "            if model_type == 'openai':\n",
    "                embeddings = OpenAIEmbeddings()\n",
    "                print(\"OpenAI embeddings initialized\")\n",
    "                return embeddings\n",
    "            \n",
    "            elif model_type == 'huggingface' and HUGGINGFACE_AVAILABLE:\n",
    "                model_name = kwargs.get('model_name', 'BAAI/bge-large-en')\n",
    "                model_kwargs = kwargs.get('model_kwargs', {'device': 'cpu'})\n",
    "                encode_kwargs = kwargs.get('encode_kwargs', {'normalize_embeddings': True})\n",
    "                \n",
    "                embeddings = HuggingFaceBgeEmbeddings(\n",
    "                    model_name=model_name,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    encode_kwargs=encode_kwargs\n",
    "                )\n",
    "                print(f\"HuggingFace embeddings initialized ({model_name})\")\n",
    "                return embeddings\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown or unavailable embedding model type: {model_type}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "class VectorStoreManager:\n",
    "    \"\"\"Manages vector store operations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_vectorstore(chunks, embeddings):\n",
    "        \"\"\"Create FAISS vector store\"\"\"\n",
    "        try:\n",
    "            vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "            print(\"FAISS vector store created successfully\")\n",
    "            return vectorstore\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "class LLMManager:\n",
    "    \"\"\"Manages LLM operations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_llm(model_name='gpt-3.5-turbo'):\n",
    "        \"\"\"Create ChatOpenAI LLM wrapper\"\"\"\n",
    "        try:\n",
    "            llm = ChatOpenAI(model_name=model_name)\n",
    "            print(f\"LLM initialized ({model_name})\")\n",
    "            return llm\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing LLM: {e}\")\n",
    "            raise\n",
    "\n",
    "class QASystem:\n",
    "    \"\"\"Manages QA operations\"\"\"\n",
    "    \n",
    "    def __init__(self, llm, vectorstore):\n",
    "        self.llm = llm\n",
    "        self.vectorstore = vectorstore\n",
    "        self.chain = None\n",
    "        self._initialize_chain()\n",
    "    \n",
    "    def _initialize_chain(self):\n",
    "        \"\"\"Initialize the QA chain\"\"\"\n",
    "        try:\n",
    "            self.chain = RetrievalQAWithSourcesChain.from_llm(\n",
    "                llm=self.llm,\n",
    "                retriever=self.vectorstore.as_retriever()\n",
    "            )\n",
    "            print(\"QA chain initialized successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing QA chain: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def ask_question(self, question):\n",
    "        \"\"\"Ask a question and return answer with sources\"\"\"\n",
    "        try:\n",
    "            result = self.chain({\"question\": question}, return_only_outputs=True)\n",
    "            answer = result.get('answer', 'No answer found.')\n",
    "            sources = result.get('sources', '')\n",
    "            return answer, sources\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing chatbot system...\n",
      "Loading documents from 3 URLs...\n",
      "Successfully loaded 3 documents\n",
      "Split documents into 37 chunks\n",
      "OpenAI embeddings initialized\n",
      "FAISS vector store created successfully\n",
      "LLM initialized (gpt-3.5-turbo)\n",
      "QA chain initialized successfully\n",
      "Chatbot system initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Main workflow\n",
    "def initialize_chatbot(config):\n",
    "    \"\"\"Initialize the complete chatbot system\"\"\"\n",
    "    try:\n",
    "        print(\"Initializing chatbot system...\")\n",
    "        \n",
    "        # Step 1: Load documents\n",
    "        documents = DocumentProcessor.load_documents(config.urls)\n",
    "        if not documents:\n",
    "            raise RuntimeError(\"No documents loaded. Check your URLs or network connection.\")\n",
    "        \n",
    "        # Step 2: Split documents\n",
    "        chunks = DocumentProcessor.split_documents(\n",
    "            documents, config.chunk_size, config.chunk_overlap\n",
    "        )\n",
    "        if not chunks:\n",
    "            raise RuntimeError(\"Failed to split documents into chunks.\")\n",
    "        \n",
    "        # Step 3: Initialize embeddings\n",
    "        embeddings = EmbeddingManager.get_embeddings(config.embedding_model)\n",
    "        \n",
    "        # Step 4: Create vector store\n",
    "        vectorstore = VectorStoreManager.create_vectorstore(chunks, embeddings)\n",
    "        \n",
    "        # Step 5: Initialize LLM\n",
    "        llm = LLMManager.create_llm(config.llm_model)\n",
    "        \n",
    "        # Step 6: Create QA system\n",
    "        qa_system = QASystem(llm, vectorstore)\n",
    "        \n",
    "        print(\"Chatbot system initialized successfully!\")\n",
    "        return qa_system\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize chatbot: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize the chatbot\n",
    "try:\n",
    "    qa_system = initialize_chatbot(config)\n",
    "except Exception as e:\n",
    "    print(f\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Check your internet connection\")\n",
    "    print(\"2. Verify your OpenAI API key is valid\")\n",
    "    print(\"3. Ensure all packages are installed correctly\")\n",
    "    print(\"4. Try with different URLs if some are not accessible\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "CHATBOT READY!\n",
      "==================================================\n",
      "\n",
      "How to use:\n",
      "  - Type your question and press Enter\n",
      "  - Type 'exit' to quit\n",
      "  - Type 'help' for usage tips\n",
      "  - Type 'sources' to see available sources\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      " Thinking...\n",
      "\n",
      " Answer:\n",
      "Amazon SageMaker is a fully managed service that provides developers and data\n",
      "scientists with the ability to build, train, and deploy machine learning models\n",
      "quickly.\n",
      "\n",
      " Sources:\n",
      "https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb\n",
      "\n",
      " Thinking...\n",
      "\n",
      " Answer:\n",
      "LLM stands for Large Language Models.\n",
      "\n",
      " Sources:\n",
      "https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def interactive_qa(qa_system):\n",
    "    \"\"\"Run interactive QA session\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CHATBOT READY!\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nHow to use:\")\n",
    "    print(\"  - Type your question and press Enter\")\n",
    "    print(\"  - Type 'exit' to quit\")\n",
    "    print(\"  - Type 'help' for usage tips\")\n",
    "    print(\"  - Type 'sources' to see available sources\")\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nYour question: \").strip()\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "                \n",
    "            if query.lower() == 'exit':\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "                \n",
    "            if query.lower() == 'help':\n",
    "                print(\"\\n Help:\")\n",
    "                print(\"  - Ask questions about the content from the configured URLs\")\n",
    "                print(\"  - The chatbot will search through the loaded documents\")\n",
    "                print(\"  - Answers are based on the actual content from the websites\")\n",
    "                continue\n",
    "                \n",
    "            if query.lower() == 'sources':\n",
    "                print(\"\\n Available sources:\")\n",
    "                for i, url in enumerate(config.urls, 1):\n",
    "                    print(f\"  {i}. {url}\")\n",
    "                continue\n",
    "            \n",
    "            # Get answer\n",
    "            print(\"\\n Thinking...\")\n",
    "            answer, sources = qa_system.ask_question(query)\n",
    "            \n",
    "            # Display answer\n",
    "            print(\"\\n Answer:\")\n",
    "            wrapped_answer = textwrap.fill(answer, width=80)\n",
    "            print(wrapped_answer)\n",
    "            \n",
    "            # Display sources if available\n",
    "            if sources:\n",
    "                print(\"\\n Sources:\")\n",
    "                print(sources)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n Interrupted by user. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n Error: {e}\")\n",
    "            print(\"Please try again or type 'exit' to quit.\")\n",
    "\n",
    "# Start interactive session\n",
    "if 'qa_system' in locals():\n",
    "    interactive_qa(qa_system)\n",
    "else:\n",
    "    print(\"Chatbot not initialized. Please run the previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
